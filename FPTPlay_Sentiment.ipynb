{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "import json\n",
    "import geoip2.database\n",
    "from datetime import datetime\n",
    "import time\n",
    "from transformers import AutoTokenizer, RobertaForSequenceClassification, AutoModelForTokenClassification, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"mongodb_comments.json\",mode=\"r\",errors= \"ignore\") as file:\n",
    "#     Data = json.load(file)\n",
    "# data = pd.read_json(\"mongodb_comments.json\", encoding= 'utf-8')\n",
    "# data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for retrieving data from MongoDB\n",
    "def get_data(n_records):\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    db = client['Sentiment_Analysis']\n",
    "    collection = db['FPTPlay_Data']\n",
    "    cursor = collection.find().limit(n_records)\n",
    "    data = pd.DataFrame(list(cursor))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>like</th>\n",
       "      <th>user_email</th>\n",
       "      <th>comment_on</th>\n",
       "      <th>ip</th>\n",
       "      <th>user_fullname</th>\n",
       "      <th>publish_status</th>\n",
       "      <th>content</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>review_status</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>device</th>\n",
       "      <th>comment_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>464817</td>\n",
       "      <td>12</td>\n",
       "      <td>13@fpt.com.vn</td>\n",
       "      <td>event</td>\n",
       "      <td>42.116.8.164</td>\n",
       "      <td>tuyen tuyen</td>\n",
       "      <td>1</td>\n",
       "      <td>chuong trinh hay qua</td>\n",
       "      <td>vuluc88</td>\n",
       "      <td>1</td>\n",
       "      <td>1431513137</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>lokiheero@gmail.com</td>\n",
       "      <td>vod</td>\n",
       "      <td>42.116.8.164</td>\n",
       "      <td>Quốc Trung</td>\n",
       "      <td>1</td>\n",
       "      <td>hehe không biết chừng nào có tập 6</td>\n",
       "      <td>vuluc88</td>\n",
       "      <td>1</td>\n",
       "      <td>1431513337</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>424467</td>\n",
       "      <td>1</td>\n",
       "      <td>n_x_cuong@yahoo.com</td>\n",
       "      <td>vod</td>\n",
       "      <td>None</td>\n",
       "      <td>nguyen xuan cuong</td>\n",
       "      <td>1</td>\n",
       "      <td>lồng tiếng mà như tập đọc, nghe không nuốt nổi...</td>\n",
       "      <td>vuluc88</td>\n",
       "      <td>1</td>\n",
       "      <td>1431525776</td>\n",
       "      <td>web-playfpt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3839</td>\n",
       "      <td>4</td>\n",
       "      <td>xxx@gmail.com</td>\n",
       "      <td>vod</td>\n",
       "      <td>42.116.8.164</td>\n",
       "      <td>xxxxxurruyu</td>\n",
       "      <td>1</td>\n",
       "      <td>phim hay qua</td>\n",
       "      <td>vuluc88</td>\n",
       "      <td>1</td>\n",
       "      <td>1431566632</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>527201</td>\n",
       "      <td>5</td>\n",
       "      <td>cloudytang@gmail.com</td>\n",
       "      <td>vod</td>\n",
       "      <td>42.116.8.164</td>\n",
       "      <td>Đàm Ngọc Vân</td>\n",
       "      <td>1</td>\n",
       "      <td>hay qua do</td>\n",
       "      <td>vuluc88</td>\n",
       "      <td>1</td>\n",
       "      <td>1431573623</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  like            user_email comment_on            ip  \\\n",
       "0   464817    12         13@fpt.com.vn      event  42.116.8.164   \n",
       "1       28     4   lokiheero@gmail.com        vod  42.116.8.164   \n",
       "2   424467     1   n_x_cuong@yahoo.com        vod          None   \n",
       "3     3839     4         xxx@gmail.com        vod  42.116.8.164   \n",
       "4   527201     5  cloudytang@gmail.com        vod  42.116.8.164   \n",
       "\n",
       "       user_fullname  publish_status  \\\n",
       "0        tuyen tuyen               1   \n",
       "1         Quốc Trung               1   \n",
       "2  nguyen xuan cuong               1   \n",
       "3        xxxxxurruyu               1   \n",
       "4       Đàm Ngọc Vân               1   \n",
       "\n",
       "                                             content reviewer  review_status  \\\n",
       "0                               chuong trinh hay qua  vuluc88              1   \n",
       "1                 hehe không biết chừng nào có tập 6  vuluc88              1   \n",
       "2  lồng tiếng mà như tập đọc, nghe không nuốt nổi...  vuluc88              1   \n",
       "3                                       phim hay qua  vuluc88              1   \n",
       "4                                         hay qua do  vuluc88              1   \n",
       "\n",
       "    timestamp       device  comment_status  \n",
       "0  1431513137      android               0  \n",
       "1  1431513337      android               0  \n",
       "2  1431525776  web-playfpt               0  \n",
       "3  1431566632      android               0  \n",
       "4  1431573623      android               0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(columns=[\"_id\",\"ranking\", \"layer\", \"object_id\", \"report_count\", \"report_reason\", \"dislike\", \"device_id\"], inplace=True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate new column called \"province\" that identifies which province user is lives in based on ip adress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ho Chi Minh', 'Invalid IP', 'Đồng Nai Province',\n",
       "       'Lâm Đồng Province', None, 'Thái Bình Province', 'Hanoi',\n",
       "       'Can Tho', 'Tây Ninh Province', 'Bến Tre Province', 'Da Nang',\n",
       "       'Nghệ An Province', 'Quảng Bình Province', 'Lào Cai Province',\n",
       "       'Bắc Ninh Province', 'Bình Định Province', 'Hải Dương Province',\n",
       "       'Quảng Nam Province', 'Haiphong', 'Thái Nguyên Province',\n",
       "       'Nam Định Province', 'Vĩnh Phúc Province', 'Đắk Lắk',\n",
       "       'Bình Phước Province', 'Bình Dương Province', 'Long An Povince',\n",
       "       'Khánh Hòa Province', 'Bắc Giang Province', 'Hòa Bình Province',\n",
       "       'Quảng Ninh', 'Trà Vinh Province', 'Hưng Yên Province',\n",
       "       'Bà Rịa–Vũng Tàu Province', 'Thừa Thiên Huế Province',\n",
       "       'Quảng Ngãi Province', 'Thanh Hóa Province', 'Vĩnh Long Province',\n",
       "       'Bạc Liêu Province', 'An Giang Province', 'Hà Nam Province',\n",
       "       'Kiên Giang Province', 'Đồng Tháp Province', 'Tiền Giang',\n",
       "       'Hậu Giang', 'Cà Mau Province', 'Kon Tum', 'Tuyên Quang Province',\n",
       "       'Hà Giang Province', 'Nakhon Pathom', 'Ninh Bình Province',\n",
       "       'New Taipei City', 'Queensland', 'Gia Lai Province', 'Osaka',\n",
       "       'Lạng Sơn Province', 'Tokyo', 'Hsinchu City', 'Karnataka',\n",
       "       'Kuala Lumpur', 'Jeollabuk-do', 'Taipei City', 'Guangdong',\n",
       "       'Seoul', 'Taoyuan', 'Western Australia', 'Aichi', 'Shiga', 'Chiba',\n",
       "       'Texas', 'Lai Châu Province', 'Phú Yên Province', 'Victoria',\n",
       "       'Phú Thọ Province', 'Taichung City', 'Odessa', 'Đăk Nông Province',\n",
       "       'Miyagi', 'Gyeongsangnam-do', 'Phuket', 'Kaohsiung', 'Kyiv Oblast',\n",
       "       'Bắc Kạn Province', 'Kyoto', 'Saitama', 'Shimane', 'Bangkok',\n",
       "       'Riyadh Region', 'Miaoli', 'Aisne', 'Yilan County', 'Hiroshima',\n",
       "       'Busan', 'California', 'Metro Manila', 'Mie', 'Birmingham',\n",
       "       'Paris', 'Ibaraki', 'Alberta', 'Hyōgo', 'Benešov District',\n",
       "       'Kanagawa', 'Dubai', 'Illinois', 'Selangor', 'Luton',\n",
       "       'North Rhine-Westphalia', 'Incheon', 'Nara', 'Gunma', 'Changhua',\n",
       "       'Okayama', 'Capital Region', 'Vientiane Prefecture', 'Colorado',\n",
       "       'Washington', 'Maryland', 'Sơn La Province', 'Florida', 'Nagasaki',\n",
       "       'Gyeonggi-do', 'Hokkaido', 'Sóc Trăng Province', 'Shizuoka',\n",
       "       'Ulsan', 'Quảng Trị Province', 'Yamanashi', 'Saxony', 'Moscow',\n",
       "       'Bình Thuận Province', 'Niigata', 'New South Wales', 'Nagano',\n",
       "       'Land Berlin', 'Virginia', 'Beijing', 'Haringey', 'Tainan',\n",
       "       'South Denmark', 'Chiang Mai', 'Rheinland-Pfalz', 'Slough',\n",
       "       'Bavaria', 'Tochigi', 'Sandwell', 'Upper Savoy', 'Uusimaa',\n",
       "       'Gyeongsangbuk-do', 'Kumamoto', 'Phnom Penh', 'North Holland',\n",
       "       'Maharashtra', 'Gifu', 'Tokushima', 'Udon Thani', 'New York',\n",
       "       'Jiangsu', 'Sakon Nakhon', 'Hawaii', 'Canterbury',\n",
       "       'British Columbia', 'North Carolina', 'Fukuoka', 'Prague', 'Ehime',\n",
       "       'Cao Bằng Province', 'Điện Biên Province', 'Chungcheongnam-do',\n",
       "       'Georgia', 'Michigan', 'Mecca Region', 'Jeollanam-do',\n",
       "       'Luanda Province', 'Primorye', 'Yunlin', 'Nampula',\n",
       "       'Hsinchu County', 'Okinawa', 'Saga', 'Fukushima',\n",
       "       'North Chungcheong', 'Yên Bái Province', 'Guangxi'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_path = r'E:\\hienntt\\github\\PROJECT\\UNIACE_project\\GeoLite2-City.mmdb'\n",
    "def get_province(ip):\n",
    "    with geoip2.database.Reader(database_path) as reader:\n",
    "        try:\n",
    "            response = reader.city(ip)\n",
    "            province = response.subdivisions.most_specific.name\n",
    "            return province\n",
    "        except:\n",
    "            return \"Invalid IP\"\n",
    "data[\"province\"] = data[\"ip\"].apply(lambda x: get_province(x) if x != np.nan else None)\n",
    "data[\"province\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert \"timestamp\" column to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2015-05-13 10:32:17\n",
       "1       2015-05-13 10:35:37\n",
       "2       2015-05-13 14:02:56\n",
       "3       2015-05-14 01:23:52\n",
       "4       2015-05-14 03:20:23\n",
       "                ...        \n",
       "99995   2016-05-23 10:10:15\n",
       "99996   2016-05-23 10:10:59\n",
       "99997   2016-05-23 10:24:57\n",
       "99998   2016-05-23 11:06:44\n",
       "99999   2016-05-23 11:13:04\n",
       "Name: date_time, Length: 100000, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"date_time\"] = pd.to_datetime(data[\"timestamp\"], unit= \"s\")\n",
    "data[\"date_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"timestamp\", axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because my device's configuration is weak, I only get 100,000  records for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 30000 entries, 0 to 43509\n",
      "Data columns (total 14 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   user_id         30000 non-null  int64         \n",
      " 1   like            30000 non-null  int64         \n",
      " 2   user_email      30000 non-null  object        \n",
      " 3   comment_on      30000 non-null  object        \n",
      " 4   ip              30000 non-null  object        \n",
      " 5   user_fullname   30000 non-null  object        \n",
      " 6   publish_status  30000 non-null  int64         \n",
      " 7   content         30000 non-null  object        \n",
      " 8   reviewer        30000 non-null  object        \n",
      " 9   review_status   30000 non-null  int64         \n",
      " 10  device          30000 non-null  object        \n",
      " 11  comment_status  30000 non-null  int64         \n",
      " 12  province        30000 non-null  object        \n",
      " 13  date_time       30000 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(5), object(8)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "new_data = data[(data[\"province\"] != \"Invalid IP\") & (data[\"province\"].notnull())][0:30000]\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Sentiment analysis for \"content\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, ensure the 'content' column is of type str\n",
    "new_data['content'] = new_data['content'].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, I do some basic text cleaning steps for improving performance.\n",
    "#Function for basic text normalization\n",
    "def normalize_content(content):\n",
    "    content = content.lower()  # Lowercase\n",
    "    content = re.sub(r'\\s+', ' ', content)  # Replace multiple spaces with a single space\n",
    "    content = re.sub(r'[^\\w\\s]', '', content)  # Remove punctuation\n",
    "    return content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the 'content' column\n",
    "new_data[\"content\"] = new_data[\"content\"].apply(normalize_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hungl\\AppData\\Local\\Temp\\ipykernel_29232\\3767677497.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  max_lengths = new_data.applymap(lambda x: len(str(x)) if pd.notnull(x) else 0).max()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id             6\n",
      "like                3\n",
      "user_email         45\n",
      "comment_on          7\n",
      "ip                 15\n",
      "user_fullname      53\n",
      "publish_status      1\n",
      "content           878\n",
      "reviewer           14\n",
      "review_status       1\n",
      "device              8\n",
      "comment_status      1\n",
      "province           24\n",
      "date_time          19\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "max_lengths = new_data.applymap(lambda x: len(str(x)) if pd.notnull(x) else 0).max()\n",
    "print(max_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As data is in Vietnamese, I will use the PhoBERT model for sentiment analysis. \n",
    "The PhoBERT model is a RoBERTa-based model that is pre-trained on a large corpus of Vietnamese text. It is available in the transformers library. \n",
    "PhoBERT is a strong model for Vietnamese text, but it was trained on text with full diacritics. Then it is well-suited for Vietnamese text with accents. For text without accents, I pre-process the data to match the model's input requirements with Transformer model for inserting Vietnamese accent marks. This model inserts accent marks (diacritics) for Vietnamese texts that don't have them (or texts with some words accented and some not).\n",
    "Link of Transformer model for inserting Vietnamese accent marks. https://huggingface.co/peterhung/vietnamese-accent-marker-xlm-roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e635549d5ae949ef8db5daf09cbae17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "# Load Accent Marker Model\n",
    "def load_accent_marker_model():\n",
    "    model_path = \"peterhung/vietnamese-accent-marker-xlm-roberta\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, add_prefix_space=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "accent_model, accent_tokenizer = load_accent_marker_model()\n",
    "\n",
    "# Load Sentiment Analysis Model\n",
    "def load_sentiment_model():\n",
    "    model_path = \"wonrax/phobert-base-vietnamese-sentiment\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "sentiment_model, sentiment_tokenizer = load_sentiment_model()\n",
    "\n",
    "# Set device for faster processing\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "accent_model.to(device)\n",
    "sentiment_model.to(device)\n",
    "accent_model.eval()\n",
    "sentiment_model.eval()\n",
    "\n",
    "# Load the tags set file for accenting\n",
    "def _load_tags_set(fpath):\n",
    "    labels = []\n",
    "    with open(fpath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                labels.append(line)\n",
    "    return labels\n",
    "\n",
    "label_list = _load_tags_set(r'C:\\Users\\hungl\\Desktop\\ThuHien\\selected_tags_names.txt')\n",
    "\n",
    "# Supporting functions for merging tokens and applying accents\n",
    "TOKENIZER_WORD_PREFIX = \"▁\"\n",
    "\n",
    "def merge_tokens_and_preds(tokens, predictions): \n",
    "    merged_tokens_preds = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        tok = tokens[i]\n",
    "        label_indexes = set([predictions[i]])\n",
    "        if tok.startswith(TOKENIZER_WORD_PREFIX):\n",
    "            tok_no_prefix = tok[len(TOKENIZER_WORD_PREFIX):]\n",
    "            cur_word_toks = [tok_no_prefix]\n",
    "            j = i + 1\n",
    "            while j < len(tokens):\n",
    "                if not tokens[j].startswith(TOKENIZER_WORD_PREFIX):\n",
    "                    cur_word_toks.append(tokens[j])\n",
    "                    label_indexes.add(predictions[j])\n",
    "                    j += 1\n",
    "                else:\n",
    "                    break\n",
    "            cur_word = ''.join(cur_word_toks)\n",
    "            merged_tokens_preds.append((cur_word, label_indexes))\n",
    "            i = j\n",
    "        else:\n",
    "            merged_tokens_preds.append((tok, label_indexes))\n",
    "            i += 1\n",
    "    return merged_tokens_preds\n",
    "\n",
    "def get_accented_words(merged_tokens_preds, label_list):\n",
    "    accented_words = []\n",
    "    for word_raw, label_indexes in merged_tokens_preds:\n",
    "        for label_index in label_indexes:\n",
    "            tag_name = label_list[int(label_index)]\n",
    "            raw, vowel = tag_name.split(\"-\")\n",
    "            if raw and raw in word_raw:\n",
    "                word_accented = word_raw.replace(raw, vowel)\n",
    "                break\n",
    "        else:\n",
    "            word_accented = word_raw\n",
    "        accented_words.append(word_accented)\n",
    "    return accented_words\n",
    "\n",
    "def insert_accents(text, model, tokenizer, label_list):\n",
    "    our_tokens = text.strip().split()\n",
    "    inputs = tokenizer(our_tokens, is_split_into_words=True, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs['input_ids']\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])[1:-1]  # exclude <s> and </s>\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs.to(device)\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    predictions = np.argmax(outputs[\"logits\"].cpu().numpy(), axis=2)[0][1:-1]\n",
    "    merged_tokens_preds = merge_tokens_and_preds(tokens, predictions)\n",
    "    accented_words = get_accented_words(merged_tokens_preds, label_list)\n",
    "    \n",
    "    return ' '.join(accented_words)\n",
    "\n",
    "def sentiment_analysis(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=None)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs.to(device))\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        return torch.argmax(probs, dim=-1).item()\n",
    "\n",
    "def process_data(df, batch_size=1000):\n",
    "    # Initialize columns\n",
    "    df[\"accented_content\"] = \"\"\n",
    "    df[\"sentiment\"] = \"\"\n",
    "\n",
    "    accented_comments = []\n",
    "    sentiments = []\n",
    "    total_batches = (len(df) + batch_size - 1) // batch_size  # Calculate number of batches\n",
    "\n",
    "    for start in tqdm(range(0, len(df), batch_size), desc=\"Processing\", total=total_batches):\n",
    "        end = min(start + batch_size, len(df))\n",
    "        batch = df[\"content\"][start:end]\n",
    "\n",
    "        # Process the batch\n",
    "        batch_accented_comments = [insert_accents(comment, accent_model, accent_tokenizer, label_list) for comment in batch]\n",
    "        batch_sentiments = [sentiment_analysis(comment, sentiment_model, sentiment_tokenizer) for comment in batch_accented_comments]\n",
    "\n",
    "        accented_comments.extend(batch_accented_comments)\n",
    "        sentiments.extend(batch_sentiments)\n",
    "        \n",
    "    # Store the results in the DataFrame\n",
    "    df[\"accented_content\"] = accented_comments\n",
    "    df[\"sentiment\"] = sentiments\n",
    "\n",
    "    return df\n",
    "\n",
    "# Run the processing\n",
    "processed_data = process_data(new_data)\n",
    "\n",
    "new_data.to_csv(\"FPTPlay_Sentiment.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change type data to string and convert it with \"utf-8\" to ensure the font is not wrong\n",
    "new_data = new_data.astype(str).apply(lambda x: x.str.encode('utf-8').str.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'likes', 'user_email', 'comment_on', 'ip', 'user_fullname',\n",
       "       'publish_status', 'content', 'reviewer', 'review_status', 'device',\n",
       "       'comment_status', 'province', 'date_time', 'accented_content',\n",
       "       'sentiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make sure the column name is not the same as special word in SQL sever\n",
    "new_data = new_data.rename(columns={\"like\":\"likes\"})\n",
    "new_data.columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data into SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlalchemy, pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Your existing connection string and connection\n",
    "SERVER = \".\"\n",
    "DATABASE = \"Sentiment_Analysis\"\n",
    "connection_string = f'DRIVER={{ODBC Driver 13 for SQL Server}};SERVER={SERVER};DATABASE={DATABASE};Trusted_Connection=yes;'\n",
    "conn = pyodbc.connect(connection_string)\n",
    "\n",
    "# Create SQLAlchemy engine using the pyodbc connection\n",
    "connection_url = sqlalchemy.engine.URL.create(\n",
    "    \"mssql+pyodbc\",\n",
    "    query={\"odbc_connect\": connection_string}\n",
    ")\n",
    "engine = create_engine(connection_url)\n",
    "\n",
    "new_data.to_sql(\"FPTPlay_Sentiment\", engine, if_exists='append', index=False, dtype={\n",
    "    'user_id': sqlalchemy.types.NVARCHAR(length=50),\n",
    "    'likes': sqlalchemy.types.NVARCHAR(length=30),\n",
    "    'user_email': sqlalchemy.types.NVARCHAR(length=200),\n",
    "    'comment_on': sqlalchemy.types.NVARCHAR(length=100),\n",
    "    'ip': sqlalchemy.types.NVARCHAR(length=100),\n",
    "    'user_fullname': sqlalchemy.types.NVARCHAR(length=200),\n",
    "    'publish_status': sqlalchemy.types.NVARCHAR(length=30),\n",
    "    'content': sqlalchemy.types.NVARCHAR(length=1000),\n",
    "    'reviewer': sqlalchemy.types.NVARCHAR(length=100),\n",
    "    'review_status': sqlalchemy.types.NVARCHAR(length=30),\n",
    "    'device': sqlalchemy.types.NVARCHAR(length=100),\n",
    "    'comment_status': sqlalchemy.types.NVARCHAR(length=30),\n",
    "    'province': sqlalchemy.types.NVARCHAR(length=100),\n",
    "    'date_time': sqlalchemy.types.NVARCHAR(length=100),\n",
    "    'accented_content': sqlalchemy.types.NVARCHAR(length=1000),\n",
    "    'sentiment': sqlalchemy.types.NVARCHAR(length=30)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  user_id likes            user_email comment_on            ip user_fullname  \\\n",
      "0  464817    12         13@fpt.com.vn      event  42.116.8.164   tuyen tuyen   \n",
      "1      28     4   lokiheero@gmail.com        vod  42.116.8.164    Quốc Trung   \n",
      "2    3839     4         xxx@gmail.com        vod  42.116.8.164   xxxxxurruyu   \n",
      "3  527201     5  cloudytang@gmail.com        vod  42.116.8.164  Đàm Ngọc Vân   \n",
      "4  527201     0  cloudytang@gmail.com        vod  42.116.8.164  Đàm Ngọc Vân   \n",
      "\n",
      "  publish_status                             content reviewer review_status  \\\n",
      "0              1                chuong trinh hay qua  vuluc88             1   \n",
      "1              1  hehe không biết chừng nào có tập 6  vuluc88             1   \n",
      "2              1                        phim hay qua  vuluc88             1   \n",
      "3              1                          hay qua do  vuluc88             1   \n",
      "4              0                         comment cai  vuluc88             1   \n",
      "\n",
      "    device comment_status     province            date_time  \\\n",
      "0  android              0  Ho Chi Minh  2015-05-13 10:32:17   \n",
      "1  android              0  Ho Chi Minh  2015-05-13 10:35:37   \n",
      "2  android              0  Ho Chi Minh  2015-05-14 01:23:52   \n",
      "3  android              0  Ho Chi Minh  2015-05-14 03:20:23   \n",
      "4  android              2  Ho Chi Minh  2015-05-14 03:21:02   \n",
      "\n",
      "                     accented_content sentiment  \n",
      "0                chương trình hay quá         1  \n",
      "1  hehe không biết chừng nào có tập 6         1  \n",
      "2                        phim hay quá         1  \n",
      "3                          hãy quá dở         0  \n",
      "4                         comment cái         2  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM FPTPlay_Sentiment\", con = engine)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close() #Release resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data loaded into SQL server again"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
